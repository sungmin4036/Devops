![image](https://user-images.githubusercontent.com/62640332/155844948-73a1a7ff-b4b3-4ec4-8c1a-062669a890c7.png)

쿠버네티스 클러스터는 애플리케이션 컨테이너를 실행하기 위한 일련의 노드 머신입니다. 쿠버네티스를 실행 중이라면 클러스터를 실행하고 있는 것입니다.

최소 수준에서 클러스터는 컨트롤 플레인 및 하나 이상의 컴퓨팅 머신 또는 노드를 포함하고 있습니다. 

컨트롤 플레인은 어느 애플리케이션을 실행하고 애플리케이션이 어느 컨테이너 이미지를 사용할지와 같이 클러스터를 원하는 상태로 유지 관리합니다. 

노드는 애플리케이션과 워크로드를 실제로 실행합니다.

클러스터는 쿠버네티스의 핵심 장점입니다. 

즉 물리 머신, 가상 머신, 온프레미스, 클라우드에 구애받지 않고 머신 그룹 전체에서 컨테이너를 예약하고 실행할 수 있습니다. 

쿠버네티스 컨테이너는 개별 머신에 연결되지 않습니다. 대신에 클러스터 전체에서 추상화됩니다.



쿠버네티스 클러스터는 컨트롤 플레인(마스터 노드)와 워커 노드로 이루어집니다. 


ㅁ 컨트롤 플레인


![image](https://user-images.githubusercontent.com/62640332/155851583-8ac6a3be-629a-4b5c-950f-5ae91fe00867.png)

: 컨트롤 플레인은 쿠버네티스 클러스터의 기능을 제어하는 역할을 합니다. 컨트롤 플레인은 다음과 같은 구성요소로 이루어져 있습니다.

- API Server
- Scheduler
- Controller Manager
- etcd

컨트롤 플레인에서 해당 요소들은 개별적인 프로세스로 동작합니다.

그렇다면 각 구성 요소들은 어떤 방식으로 통신을 하게 될까요? 

정답은 '모든 구성요소는 API 서버로만 통신한다' 입니다. 

예들어 컨트롤러 매니저가 etcd의 데이터를 변경하기 위해서는 API 서버를 통해 요청을 보내야합니다.

쿠버네티스 클러스터의 기능을 제어하기 때문에 컨트롤 플레인이 제 기능을 수행하지 못한다면 쿠버네티스 클러스터도 마찬가지로 본래의 기능을 수행하지 못하게 됩니다.

따라서 컨트롤 플레인이 단일 실패 지점(SPOF)이 되지 않도록 가용성을 확보해야 합니다.

구성 요소가 같은 노드에 존재해야하는 워커 노드와 다르게 컨트롤 플레인의 구성 요소는 여러 서버에 구축될 수 있습니다. 

따라서 여러 노드에 컨트롤 플레인의 구성 요소 인스턴스를 여러개 띄워 가용성을 향상시킬 수 있습니다.

이 때 알아둬야 할 점은 etcd와 API 서버는 여러 인스턴스를 동시에 활성화시켜 병렬로 실행이 가능하지만, 스케줄러와 컨트롤러 매니저는 나머지 인스턴스는 대기 상태로 유지한다는 점입니다.



<br>
<br>
<br>


![image](https://user-images.githubusercontent.com/62640332/155851619-563d8ffd-6280-4cbe-a0c6-72343008f36a.png)


ㅁ etcd

: 쿠버네티스에서 생성된 모든 오브젝트는 상태와 메니페스트를 영속적으로 유지해야 합니다.

이를 위해서 쿠버네티스는 분산 key-value 저장소인 etcd를 사용합니다. 

etcd는 분산된 아키텍처 형태를 가질 수 있으며 이를 통해 고가용성 및 빠른 성능을 제공하게 됩니다.

쿠버네티스는 etcd v2나 v3를 모두 지원하지만 v3가 더 나은 성능을 보여주기 때문에 v3를 사용하는 것을 권장합니다. 

따라서 v3 기준으로 데이터가 어떻게 되는지 살펴보면 etcd는 계층적 key 구조를 통해 쿠버네티스의 데이터를 저장합니다. 

이 때, /registry 아래에 모든 데이터를 저장하게 됩니다.

<br>
<br>
<br>

ㅁ RAFT 합의 알고리즘

: 분산된 etcd 클러스터는 RAFT 합의 알고리즘을 통해 데이터의 일관성을 유지하게 됩니다. 

RAFT 합의 알고리즘을 간단하게 말하자면 클러스터에 Split Brain 현상이 발생하게 되면 과반수가 넘는 노드가 있는 쪽만 유효한 요청을 수행할 수 있다는 알고리즘입니다. 

RAFT 합의 알고리즘과 관련된 내용은 이 글을 살펴보시면 더 자세한 내용을 보실 수 있습니다. 

추가적으로 etcd 인스턴스 수를 일반적으로 홀수로 배포하게 되는데 이는 짝수인 경우 과반이 존재하지 않을 가능성이 높아지기 때문입니다.


<br>
<br>
<br>

ㅁ API Server

:API Server는 클러스터의 모든 구성요소가 다른 구성요소와 통신하기 위해 필요한 중요 구성요소입니다. 

API Server는 클러스터와 관련된 다양한 REST API를 제공합니다.

클라이언트가 API 서버에 리소스 생성 및 조회의 요청을 보내게 되면 다음과 같은 과정을 거치게 됩니다.

- 요청을 보낸 클라이언트가 인증된 클라이언트인지 확인
- 인증된 사용자가 현재 보낸 요청을 수행할 수 있는 권한이 있는지 확인
- (리소스 생성 및 수정, 삭제) 리소스를 기존에 정의된 플러그인을 통해 수정
- 리소스의 유효성을 확인한 후 etcd에 저장
- 리소스의 변경 사항을 리소스를 감시하고 있는 모든 클라이언트에게 통보


<br>
<br>
<br>

ㅁ Controller Manager

: Scheduler를 통해 Pod이 스케줄링 되었다면 다음으로 해야하는 작업은 해당 리소스를 원하는 상태로 만드는 작업입니다. 쿠베네티스에서 이러한 작업을 수행하는 컴포넌트를 Controller라고 합니다. 컨트롤러 매니저는 다양한 컨트롤러들을 실행하는 역할을 담당합니다.


<br>
<br>
<br>


ㅁ Controller

: Controller는 API 서버를 통해 리소스의 변경을 감시하고 변경하는 작업을 담당합니다. 클라이언트가 선언한 Spec으로 조정하며 새롬게 변경된 상태를 Status에 저장합니다. 쿠버네티스에는 기본적으로 제공되는 다양한 Controller가 존재합니다. (ex. Deployment, ReplicaSet, StatefulSet 등)

<br>
<br>
<br>

ㅁ Worker Node

: 워커 노드는 실제 어플리케이션이 실행되는 노드로 kubelet과 kube-proxy라는 구성요소를 가지고 있습니다.

<br>
<br>
<br>

ㅁ Kubelet

: Scheduler에 의해 Pod이 스케줄링되면 API 서버는 Kubelet에게 Pod을 생성하라는 요청을 보냅니다. 이 요청을 받은 Kubelet은 지정된 컨테이너 런타임과 이미지를 사용해 컨테이너를 생성합니다. 또한, 실행 중인 컨테이너를 모니터링하며 관련된 정보를 API 서버에게 보내게 됩니다. 추가로 Liveness Probe가 설정되어 있는 경우, 컨테이너를 재시작하는 역할도 kubulet에서 담당합니다.

<br>
<br>
<br>

ㅁ Kube-Proxy

: Kube-Proxy는 서비스의 IP 및 포트로 들어온 접속을 서비스의 엔드포인트에 해당하는 Pod에 연결하는 역할을 담당합니다. Proxy라는 이름이 붙은 이유는 초기 쿠버네티스 버전에서 kube-proxy는 userspace에서 동작하던 프록시였기 때문입니다. 하지만 현재는 성능이 더 우수한 iptables 프록시 모드로 수행됩니다.