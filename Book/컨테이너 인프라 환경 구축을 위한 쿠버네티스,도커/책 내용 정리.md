
---

## ㅁ 오케스트레이션

---

컨테이너 인프라 환경이란 리눅스 운영 체제의 커널 하나에서 여러 개의 컨테이너가 격리된 상태로 실행되는 인프라 환경을 말합니다.

여기서 컨테이너는 하나 이상의 목적을 위해 독립적으로 작동하는 프로세스입니다.

개인 환경에서는 1명의 관리자(사용자)가 다양한 응용프로그램을 사용하므로 각각의 프로그램을 컨테이너로 구현할 필요가 거의 없습니다. 

하지만 기업 환경에서는 다수의 관리자가 수백 또는 수천 대의 서버를 함께 관리하기 때문에 일관성을 유지하는 것이 매우 중요합니다.

이런 경우 컨테이너 인프라 환경을 구성하면 눈송이 서버(여러 사람이 만져서 설정의 일관성이 떨어진 서버)를 방지하는 데 효과적입니다.

가상화 환경에서는 각각의 가상 머신이 모두 독립적인 운영 체제 커널을 가지고 있어야 하기 때문에 그만큼 자원을 더 소모해야 하고 성능이 떨어질 수밖에 없습니다. 

하지만 컨테이너 인프라 환경은 운영 체제 커널 하나에 컨테이너 여러 개가 격리된 형태로 실행되기 때문에 자원을 효율적으로 사용할 수 있고 거치는 단계가 적어서 속도도 훨씬 빠릅니다.

![image](https://user-images.githubusercontent.com/62640332/167587623-bb332b02-f6a2-45d1-8a2c-7d2e9c3aa874.png)

### ㅁ 컨테이너 인프라 환경이 주목 받지 못했던 이유

---

컨테이너 인프라 환경이 처음부터 주목받았던 것은 아닙니다. 

이미 가상화 환경에서 상용 솔루션(VMware)을 이용해 안정적으로 시스템을 운용하고 있었고, 기술 성숙도가 높아 문제없이 관리되고 있었습니다. 

그러다 시간이 지나 커널을 공유해 더 많은 애플리케이션을 올릴 수 있는 컨테이너가 도입되기 시작하면서 늘어난 컨테이너를 관리해야 했습니다. 

하지만 기존의 컨테이너 관리 솔루션(Docker Swarm, Mesos, Nomad 등)들은 현업의 요구 사항을 충족시키기에는 부족한 점이 있었습니다.

그래서 컨테이너 인프라 환경이 주는 장점이 많이 있음에도 컨테이너 관리 문제 때문에 보편화되기가 어려웠습니다.

그 이후 구글이 쿠버네티스를 오픈소스로 공개하여 컨테이너 관리가 용이해짐.


<br>
<br>

쿠버네티스를 컨테이너 관리 도구라고 설명했지만, 실제로 쿠버네티스는 컨테이너 오케스트레이션을 위한 솔루션입니다. 

오케스트레이션(Orchestration)이란 복잡한 단계를 관리하고 요소들의 유기적인 관계를 미리 정의해 손쉽게 사용하도록 서비스를 제공하는 것을 의미합니다. 

다수의 컨테이너를 유기적으로 연결, 실행, 종료할 뿐만 아니라 상태를 추적하고 보존하는 등 컨테이너를 안정적으로 사용할 수 있게 만들어주는 것이 컨테이너 오케스트레이션입니다

### ㅁ 대표적인 컨테이너 오케스트레이션

---

![image](https://user-images.githubusercontent.com/62640332/167589921-015c8a2b-9527-469a-b17c-709e8a5ce09c.png)


• 도커 스웜(Docker Swarm): 간단하게 설치할 수 있고 사용하기도 용이합니다. 

그러나 그만큼 기능이 다양하지 않아 대규모 환경에 적용하려면 사용자 환경을 변경해야 할 수 있습니다. 

따라서 소규모 환경에서는 유용하지만 대규모 환경에서는 잘 사용하지 않는 편입니다.

<br>
<br>

• 메소스(Mesos): 아파치(Apache)의 오픈 소스 프로젝트로 역사와 전통이 있는 클러스터 도구이며 트위터, 에어비앤비, 애플, 우버 등 다양한 곳에서 이미 검증된 솔루션입니다. 

메소스는 2016년 DC/OS(Data Center OS, 대규모 서버 환경에서 자원을 유연하게 공유하며 하나의 자원처럼 관리하는 도구)의 지원으로 매우 간결해졌습니다. 

하지만 기능을 충분히 활용하려면 분산 관리 시스템과 연동해야 합니다. 따라서 여러 가지 솔루션을 유기적으로 구성해야 하는 부담이 있습니다.

<br>
<br>

• 노매드(Nomad): 베이그런트를 만든 해시코프(HashiCorp)사의 컨테이너 오케스트레이션으로, 베이그런트처럼 간단한 구성으로 컨테이너 오케스트레이션 환경을 제공합니다. 

하지만 도커 스웜과 마찬가지로 기능이 부족하므로 복잡하게 여러 기능을 사용하는 환경이 아닌 가볍고 간단한 기능만 필요한 환경에서 사용하기를 권장합니다. 

해시코프의 Consul(서비스 검색, 구성 및 분할 기능 제공)과 Vault(암호화 저장소)와의 연동이 원할하므로 이런 도구에 대한 사용 성숙도가 높은 조직이라면 노매드 도입을 고려해볼 수 있습니다.

<br>
<br>


• 쿠버네티스: 다른 오케스트레이션 솔루션보다는 시작하는 데 어려움이 있지만, 쉽게 사용할 수 있도록 도와주는 도구들이 있어서 설치가 쉬워지는 추세입니다. 

또한 다양한 형태의 쿠버네티스가 지속적으로 계속 발전되고 있어서 컨테이너 오케스트레이션을 넘어 IT 인프라 자체를 컨테이너화하고, 컨테이너화된 인프라 제품군을 쿠버네티스 위에서 동작할 수 있게 만듭니다. 

즉 거의 모든 벤더와 오픈 소스 진영 모두에서 쿠버네티스를 지원하고 그에 맞게 통합 개발하고 있습니다. 

그러므로 컨테이너 오케이스트레이션을 학습하거나 도입하려고 한다면 쿠버네티스를 우선적으로 고려해야 합니다.

![image](https://user-images.githubusercontent.com/62640332/167590383-25eb6435-ace8-4906-ba6a-81b7dcaa280c.png)

<br>
<br>


### ㅁ 쿠버네티스 구성하는 방법

---

1. 퍼블릭 클라우드 업체에서 제공하는 관리형 쿠버네티스인 EKS(Amazon Elastic Kubernetes Service), AKS(Azure Kubernetes Services), GKE(Google Kubernetes Engine) 등을 사용합니다. 

구성이 이미 다 갖춰져 있고 마스터 노드를 클라우드 업체에서 관리하기 때문에 학습용으로는 적합하지 않습니다.
 
<br>

2. 수세의 Rancher, 레드햇의 OpenShift와 같은 플랫폼에서 제공하는 설치형 쿠버네티스를 사용합니다. 

하지만 유료라 쉽게 접근하기 어렵습니다.

<br>

3. 사용하는 시스템에 쿠버네티스 클러스터를 자동으로 구성해주는 솔루션을 사용합니다. 주요 솔루션으로는 kubeadm, kops(Kubernetes Operations), KRIB(Kubernetes Rebar Integrated Bootstrap), kubespray가 있습니다. 

4가지의 주요 솔루션 중에 kubeadm이 가장 널리 알려져 있습니다. kubeadm은 사용자가 변경하기도 수월하고, 온프레미스(On-Premises)와 클라우드를 모두 지원하며, 배우기도 쉽습니다. 

이러한 솔루션들을 `구성형 쿠버네티스`라고 합니다.

![image](https://user-images.githubusercontent.com/62640332/167782538-304d74f9-fea9-4be4-8a61-d12be53091b1.png)

<br>
<br>

```
Tip ☆ 쿠버네티스 구성 요소의 이름 생성 규칙
    
쿠버네티스의 구성 요소는 동시에 여러 개가 존재하는 경우 중복된 이름을 피하려고 뒤에 해시(hash) 코드가 삽입됩니다. 
이때 해시 코드는 무작위 문자열로 생성됩니다.
```

![image](https://user-images.githubusercontent.com/62640332/167785690-9ba5892d-0d5c-4961-8413-d14801b9243b.png)

coredns에는 중간에 5644d7b6d9라는 문자열이 하나 더 있는데, 이는 레플리카셋(ReplicaSet)을 무작위 문자열로 변형해 추가한 것입니다. calico-kube-controllers도 같은 경우입니다.

![image](https://user-images.githubusercontent.com/62640332/167785776-3098f442-68b2-45d6-a03b-566dcbf54bff.png)

<br>
<br>

- 관리자나 개발자가 파드를 배포할 때

![image](https://user-images.githubusercontent.com/62640332/167785874-a6a6f5c6-5737-409a-812f-830675e64add.png)

<br>
<br>

### ㅁ 마스터 노드

---

- kubectl: 쿠버네티스 클러스터에 명령을 내리는 역할을 합니다. 
 
다른 구성 요소들과 다르게 바로 실행되는 명령 형태인 바이너리(binary)로 배포되기 때문에 마스터 노드에 있을 필요는 없습니다.

하지만 통상적으로 API 서버와 주로 통신하므로 이 책에서는 API 서버가 위치한 마스터 노드에 구성했습니다.

<br>

➊ API 서버: 쿠버네티스 클러스터의 중심 역할을 하는 통로입니다. 

주로 상태 값을 저장하는 etcd와 통신하지만, 그 밖의 요소들 또한 API 서버를 중심에 두고 통신하므로 API 서버의 역할이 매우 중요합니다. 

회사에 비유하면 모든 직원과 상황을 관리하고 목표를 설정하는 관리자에 해당합니다.

<br>
 
➋ etcd: 구성 요소들의 상태 값이 모두 저장되는 곳입니다. 회사의 관리자가 모든 보고 내용을 기록하는 노트라고 생각하면 됩니다. 

실제로 etcd 외의 다른 구성 요소는 상태 값을 관리하지 않습니다. 

그러므로 etcd의 정보만 백업돼 있다면 긴급한 장애 상황에서도 쿠버네티스 클러스터는 복구할 수 있습니다. 

또한 etcd는 분산 저장이 가능한 key-value 저장소이므로, 복제해 여러 곳에 저장해 두면 하나의 etcd에서 장애가 나더라도 시스템의 가용성을 확보할 수 있습니다. 

이와 같은 멀티 마스터 노드 형태는 부록에서 kubespray로 구성해 보겠습니다.

```
Tip ☆ etcd의 의미

etcd(엣시디)를 약어로 오인하는 경우가 있습니다. 
etcd는 리눅스의 구성 정보를 주로 가지고 있는 etc 디렉터리와 distributed(퍼뜨렸다)의 합성어입니다.
따라서 etcd는 구성 정보를 퍼뜨려 저장하겠다는 의미입니다.
```  

<br>

➌ 컨트롤러 매니저: 컨트롤러 매니저는 쿠버네티스 클러스터의 오브젝트 상태를 관리합니다. 

예를 들어 워커 노드에서 통신이 되지 않는 경우, 상태 체크와 복구는 컨트롤러 매니저에 속한 노드 컨트롤러에서 이루어집니다. 

다른 예로 레플리카셋 컨트롤러는 레플리카셋에 요청받은 파드 개수대로 파드를 생성합니다. 

뒤에 나오는 서비스와 파드를 연결하는 역할을 하는 엔드포인트 컨트롤러 또한 컨트롤러 매니저입니다.

이와 같이 다양한 상태 값을 관리하는 주체들이 컨트롤러 매니저에 소속돼 각자의 역할을 수행합니다. 

여기서 나온 오브젝트에 관해서는 ‘3.2.2 오브젝트란’에서 자세히 다룹니다.

<br>

➍ 스케줄러: 노드의 상태와 자원, 레이블, 요구 조건 등을 고려해 파드를 어떤 워커 노드에 생성할 것인지를 결정하고 할당합니다. 

스케줄러라는 이름에 걸맞게 파드를 조건에 맞는 워커 노드에 지정하고, 파드가 워커 노드에 할당되는 일정을 관리하는 역할을 담당합니다.

<br>
<br>

### ㅁ 워커 노드

<br>

➎ kubelet: 파드의 구성 내용(PodSpec)을 받아서 컨테이너 런타임으로 전달하고, 파드 안의 컨테이너들이 정상적으로 작동하는지 모니터링합니다.

<br>

➏ 컨테이너 런타임(CRI, Container Runtime Interface): 파드를 이루는 컨테이너의 실행을 담당합니다. 

파드 안에서 다양한 종류의 컨테이너가 문제 없이 작동하게 만드는 표준 인터페이스입니다. 

자세한 내용은 ‘부록 D 컨테이너 깊게 들여다보기’를 참고하기 바랍니다.

<br>

➐ 파드(Pod): 한 개 이상의 컨테이너로 단일 목적의 일을 하기 위해서 모인 단위입니다. 

즉, 웹 서버 역할을 할 수도 있고 로그나 데이터를 분석할 수도 있습니다. 

여기서 중요한 것은 파드는 언제라도 죽을 수 있는 존재라는 점입니다. 

이것이 쿠버네티스를 처음 배울 때 가장 이해하기 어려운 부분입니다. 

가상 머신은 언제라도 죽을 수 있다고 가정하고 디자인하지 않지만, 파드는 언제라도 죽을 수 있다고 가정하고 설계됐기 때문에 쿠버네티스는 여러 대안을 디자인했습니다. 

어려운 내용이므로 여러 가지 테스트를 통해 여러분이 이해하도록 돕겠습니다.

  
    
 ![image](https://user-images.githubusercontent.com/62640332/167788296-b9fe3509-b218-4e15-8073-bd54ad910703.png)


<br>
<br>

### ㅁ 선택 가능한 구성 요소

0 번부터  7 번까지는 기본 설정으로 배포된 쿠버네티스에서 이루어지는 통신 단계를 구분한 것입니다. 

이외에 선택적으로 배포하는 것들은 순서와 상관이 없기 때문에 10번대로 구분해 표시했습니다. 

선택 가능한 부가 요소들은 이 책에서 다루기에는 너무 깊은 내용이라 이런 요소가 있다는 정도만 알면 충분합니다.

<br>

⓫ 네트워크 플러그인: 쿠버네티스 클러스터의 통신을 위해서 네트워크 플러그인을 선택하고 구성해야 합니다. 

네트워크 플러그인은 일반적으로 CNI로 구성하는데, 주로 사용하는 CNI에는 캘리코(Calico), 플래널(Flannel), 실리움(Cilium), 큐브 라우터(Kube-router), 로마나(Romana), 위브넷(WeaveNet), Canal이 있습니다. 여기서는 캘리코를 선택해 구성했습니다.

<br>

⓬ CoreDNS: 클라우드 네이티브 컴퓨팅 재단에서 보증하는 프로젝트로, 빠르고 유연한 DNS 서버입니다. 

쿠버네티스 클러스터에서 도메인 이름을 이용해 통신하는 데 사용하며, 6장에서 간단히 사용해 볼 예정입니다. 

실무에서 쿠버네티스 클러스터를 구성하여 사용할 때는 IP보다 도메인 네임을 편리하게 관리해 주는 CoreDNS를 사용하는 것이 일반적입니다. 

해당 내용을 자세히 알아보려면 홈페이지(https://coredns.io)를 참조하기 바랍니다.

```  
Tip ☆ CNI

CNI(Container Network Interface, 컨테이너 네트워크 인터페이스)는 클라우드 네이티브 컴퓨팅 재단의 프로젝트로,
컨테이너의 네트워크 안정성과 확장성을 보장하기 위해 개발됐습니다. 
CNI에 사용할 수 있는 네트워크 플러그인은 다양한데, 구성 방식과 지원하는 기능, 성능이 각기 다르므로 사용 목적에 맞게 선택하면 됩니다. 
예를 들어 Calico는 L3로 컨테이너 네트워크를 구성하고, Flannel은 L2로 컨테이너 네트워크를 구성합니다. 
또한 네트워크 프로토콜인 BGP와 VXLAN의 지원, ACL(Access Control List) 지원, 보안 기능 제공 등을 살펴보고 필요한 조건을 가지고 있는 네트워크 플러그인을 선택할 수 있어서 설계 유연성이 매우 높습니다.
```
<br>
<br>


### ㅁ 사용자가 배포된 파드에 접속할 때

---

1. kube-proxy: 쿠버네티스 클러스터는 파드가 위치한 노드에 kube-proxy를 통해 파드가 통신할 수 있는 네트워크를 설정합니다. 

이때 실제 통신은 br_netfilter와 iptables로 관리합니다. 

두 기능은 Vagrantfile에서 호출하는 config.sh 코드를 설명할 때 다뤘습니다.

<br>

2. 파드: 이미 배포된 파드에 접속하고 필요한 내용을 전달받습니다. 이때 대부분 사용자는 파드가 어느 워커 노드에 위치하는지 신경 쓰지 않아도 됩니다.

쿠버네티스의 각 구성 요소를 파드의 배포와 접속 관점에서 설명했지만, 이해하기는 쉽지 않을 겁니다. 

파드가 배포되는 과정을 살펴보며 쿠버네티스의 구성 요소를 좀 더 깊이 알아보겠습니다.

<br>

쿠버네티스의 가장 큰 장점은 쿠버네티스의 구성 요소마다 하는 일이 명확하게 구분돼 각자의 역할만 충실하게 수행하면 클러스터 시스템이 안정적으로 운영된다는 점입니다. 

이렇게 각자의 역할이 명확하게 나뉘어진 것은 마이크로서비스 아키텍처(MSA) 구조와도 밀접하게 연관됩니다. 

또한 역할이 나뉘어 있어서 문제가 발생했을 때 어느 부분에서 문제가 발생했는지 디버깅하기 쉽습니다.


<br>
<br>



### ㅁ 파드의 생명주기(life cycle)

---

![image](https://user-images.githubusercontent.com/62640332/167790485-22cbb381-5aa0-4d58-a373-b2bb744c3334.png)

1. kubectl을 통해 API 서버에 파드 생성을 요청합니다.

<br>

2. (업데이트가 있을 때마다 매번) API 서버에 전달된 내용이 있으면 API 서버는 etcd에 전달된 내용을 모두 기록해 클러스터의 상태 값을 최신으로 유지합니다. 
 
따라서 각 요소가 상태를 업데이트할 때마다 모두 API 서버를 통해 etcd에 기록됩니다.

<br>

3. API 서버에 파드 생성이 요청된 것을 컨트롤러 매니저가 인지하면 컨트롤러 매니저는 파드를 생성하고, 이 상태를 API 서버에 전달합니다. 

참고로 아직 어떤 워커 노드에 파드를 적용할지는 결정되지 않은 상태로 파드만 생성됩니다. 이 부분은 ‘3.2.2 오브젝트란’에서 보충 설명하겠습니다.

<br>

4. API 서버에 파드가 생성됐다는 정보를 스케줄러가 인지합니다. 스케줄러는 생성된 파드를 어떤 워커 노드에 적용할지 조건을 고려해 결정하고 해당 워커 노드에 파드를 띄우도록 요청합니다.

5. API 서버에 전달된 정보대로 지정한 워커 노드에 파드가 속해 있는지 스케줄러가 kubelet으로 확인합니다.

6. kubelet에서 컨테이너 런타임으로 파드 생성을 요청합니다.

7. 파드가 생성됩니다.

8. 파드가 사용 가능한 상태가 됩니다.

앞의 내용을 살펴보다가 ‘API 서버는 감시만 하는 걸까? 화살표가 반대로 그려져야 맞지 않을까?’라는 의문이 들었다면 내용을 제대로 본 겁니다.

이 부분은 쿠버네티스를 이해하는 데 매우 중요한 부분입니다. 쿠버네티스는 작업을 순서대로 진행하는 워크플로(workflow, 작업 절차) 구조가 아니라 `선언적인(declarative)` 시스템 구조를 가지고 있습니다. 

즉, 각 요소가 추구하는 상태(desired status)를 선언하면 현재 상태(current status)와 맞는지 점검하고 그것에 맞추려고 노력하는 구조로 돼 있다는 뜻입니다.

따라서 추구하는 상태를 API 서버에 선언하면 다른 요소들이 API 서버에 와서 현재 상태와 비교하고 그에 맞게 상태를 변경하려고 합니다. 

여기서 API는 현재 상태 값을 가지고 있는데, 이것을 보존해야 해서 etcd가 필요합니다. 

API 서버와 etcd는 거의 한몸처럼 움직이도록 설계됐습니다. 다만, 여기서 `워커 노드는 워크플로 구조`에 따라 설계됐습니다. 

쿠버네티스가 kubelet과 컨테이너 런타임을 통해 파드를 새로 생성하고 제거해야 하는 구조여서 선언적인 방식으로 구조화하기에는 어려움이 있기 때문입니다. 

또한 명령이 절차적으로 전달되는 방식은 시스템의 성능을 높이는 데 효율적입니다. 

하지만 마스터 노드는 이미 생성된 파드들을 유기적으로 연결하므로 쿠버네티스 `클러스터를 안정적으로 유지`하려면 선언적인 시스템이 더 낫습니다.

<br> 
<br> 

- kubectl

앞에서 kubectl은 꼭 마스터 노드에 위치할 필요 없다고 했습니다. 실제로 쿠버네티스 클러스터의 외부에서 쿠버네티스 클러스터에 명령을 내릴 수도 있습니다.

‘3.1.5 파드 생명주기로 쿠버네티스 구성 요소 살펴보기’를 보면 kubectl은 API 서버를 통해 쿠버네티스에 명령을 내립니다. 

따라서 kubectl이 어디에 있더라도 API 서버의 접속 정보만 있다면 어느 곳에서든 쿠버네티스 클러스터에 명령을 내릴 수 있습니다.

```
# 쿠버네티스 클러스터의 정보(/etc/kubernetes/admin.conf)
```

- kubelet

kubelet은 쿠버네티스에서 파드의 생성과 상태 관리 및 복구 등을 담당하는 매우 중요한 구성 요소입니다. 

따라서 kubelet에 문제가 생기면 파드가 정상적으로 관리되지 않습니다.

\# 기능을 검증하려면 실제로 파드를 배포해야 합니다.

- kube-proxy

kubelet이 파드의 상태를 관리한다면 kube-proxy는 파드의 통신을 담당합니다



### ㅁ 쿠버네티스 생성 명령어 run, create 차이점

---

run으로 파드를 생성하면 단일 파드 1개만 생성되고 관리됩니다. 

create deployment로 파드를 생성하면 디플로이먼트(Deployment)라는 관리 그룹 내에서 파드가 생성됩니다.

![image](https://user-images.githubusercontent.com/62640332/167969742-77ddc81e-e5b4-4abb-8e85-7d160390d239.png)

<br>
<br>

### ㅁ 오브젝트란

---

쿠버네티스를 사용하는 관점에서 파드와 디플로이먼트는 스펙(spec)과 상태(status) 등의 값을 가지고 있습니다. 

이러한 값을 가지고 있는 파드와 디플로이먼트를 개별 속성을 포함해 부르는 단위를 `오브젝트(Object)`라고 합니다.

<br>

1. 기본 오브젝트

• 파드(Pod): 쿠버네티스에서 실행되는 최소 단위, 즉 웹 서비스를 구동하는 데 필요한 최소 단위입니다. 

독립적인 공간과 사용 가능한 IP를 가지고 있습니다. 

하나의 파드는 1개 이상의 컨테이너를 갖고 있기 때문에 여러 기능을 묶어 하나의 목적으로 사용할 수도 있습니다. 

그러나 범용으로 사용할 때는 대부분 1개의 파드에 1개의 컨테이너를 적용합니다(차이가 조금 있으나 우선 1개라고 이해하겠습니다. 자세한 것은 4장에서 다룹니다).

<br>

• 네임스페이스(Namespaces): 쿠버네티스 클러스터에서 사용되는 리소스들을 구분해 관리하는 그룹입니다. 

예를 들어 3장에서는 3가지 네임스페이스를 사용합니다. 

> -  default: 특별히 지정하지 않으면 기본으로 할당
> -  kube-system: 쿠버네티스 시스템에서 사용되는 
> - metallb-system: 온프레미스에서 쿠버네티스를 사용할 경우 외부에서 쿠버네티스 클러스터 내부로 접속하게 도와주는 컨테이너들이 속해있음 

<br>

• 볼륨(Volume): 파드가 생성될 때 파드에서 사용할 수 있는 디렉터리를 제공합니다. 

기본적으로 파드는 영속되는 개념이 아니라 제공되는 디렉터리도 임시로 사용합니다. 

하지만 파드가 사라지더라도 저장과 보존이 가능한 디렉터리를 볼륨 오브젝트를 통해 생성하고 사용할 수 있습니다.

<br>

• 서비스(Service): 파드는 클러스터 내에서 유동적이기 때문에 접속 정보가 고정일 수 없습니다. 

따라서 파드 접속을 안정적으로 유지하도록 서비스를 통해 내/외부로 연결됩니다. 

그래서 서비스는 새로 파드가 생성될 때 부여되는 새로운 IP를 기존에 제공하던 기능과 연결해 줍니다. 

쉽게 설명하면 쿠버네티스 외부에서 쿠버네티스 내부로 접속할 때 내부가 어떤 구조로 돼 있는지, 파드가 살았는지 죽었는지 신경 쓰지 않아도 이를 논리적으로 연결하는 것이 서비스입니다. 

기존 인프라에서 로드밸런서, 게이트웨이와 비슷한 역할을 합니다. 서비스라는 이름 때문에 처음에 개념을 이해하기가 매우 어렵습니다. 

따라서 ‘3.3 쿠버네티스 연결을 담당하는 서비스’에서 집중적으로 다루겠습니다.

![image](https://user-images.githubusercontent.com/62640332/167970237-b6cd85ec-c8f5-4bf3-aeb5-8966f379a139.png)


<br>
<br>


2. 디플로이먼트

기본 오브젝트만으로도 쿠버네티스를 사용할 수 있습니다. 하지만 한계가 있어서 이를 좀 더 효율적으로 작동하도록 기능들을 조합하고 추가해 구현한 것이 디플로이먼트(Deployment)입니다

쿠버네티스에서 가장 많이 쓰이는 디플로이먼트 오브젝트는 파드에 기반을 두고 있으며, 레플리카셋 오브젝트를 합쳐 놓은 형태입니다

![image](https://user-images.githubusercontent.com/62640332/167970435-f843a24b-57ad-448a-a1ac-40d4edbbd10d.png)

실제로 API 서버와 컨트롤러 매니저는 단순히 파드가 생성되는 것을 감시하는 것이 아니라 디플로이먼트처럼 레플리카셋을 포함하는 오브젝트의 생성을 감시합니다

![image](https://user-images.githubusercontent.com/62640332/167970534-d0591fc1-1277-41d0-a1be-a1c1d16cf041.png)


### ㅁ 디플로이먼트가 필요한 이유

---

많은 사용자를 대상으로 웹 서비스를 하려면 다수의 파드가 필요한데, 이를 하나씩 생성한다면 매우 비효율적입니다. 

그래서 쿠버네티스에서는 `다수의 파드를 만드는 레플리카셋 오브젝트를 제공`합니다.

예를 들어 파드를 3개 만들겠다고 레플리카셋에 선언하면 컨트롤러 매니저와 스케줄러가 워커 노드에 파드 3개를 만들도록 선언합니다. 

![image](https://user-images.githubusercontent.com/62640332/167971781-46ca36a1-d036-4e8f-a78a-3a6afad8c445.png)

그러나 레플리카셋은 파드 수를 보장하는 기능만 제공하기 때문에 롤링 업데이트 기능 등이 추가된 `디플로이먼트를 사용해 파드 수를 관리하기를 권장`합니다

<br>

kubectl create deployment 명령으로 디플로이먼트를 생성하긴 했지만, 1개의 파드만 만들어졌을 뿐입니다. 

- 디플로이먼트를 생성하면서 한꺼번에 여러 개의 파드를 만들 순 없을까요? 

create에서는 replicas 옵션을 사용할 수 없고, scale은 이미 만들어진 디플로이먼트에서만 사용할 수 있습니다.

이런 설정을 적용하려면 필요한 내용을 파일로 작성해야 합니다. 

이때 작성하는 파일을 오브젝트 스펙(spec)이라고 합니다. 오브젝트 스펙은 일반적으로 야믈(YAML) 문법으로 작성합니다.

![image](https://user-images.githubusercontent.com/62640332/168002125-fb2b7a76-a1f7-44f2-8cfe-b45460f9b494.png)

> - run은 파드를 간단하게 생성하는 매우 편리한 방법입니다. 
하지만 run으로는 단일 파드만을 생성할 수 있습니다. 따라서 run을 모든 상황에 적용해 사용하기는 어렵습니다. 
> - create로 디플로이먼트를 생성하면 앞에서 확인한 것처럼 파일의 변경 사항을 바로 적용할 수 없다는 단점이 있습니다. 
이런 경우를 위해 쿠버네티스는 `apply`라는 명령어를 제공합니다.

![image](https://user-images.githubusercontent.com/62640332/168003524-a104f41a-e1aa-4b40-96cd-1e17db518e65.png)

<br>
<br>

쿠버네티스는 거의 모든 부분이 자동 복구되도록 설계됐습니다. 

특히 파드의 자동 복구 기술을 `셀프 힐링(Self-Healing)`이라고 하는데, 

제대로 작동하지 않는 컨테이너를 다시 시작하거나 교체해 파드가 정상적으로 작동하게 합니다

![image](https://user-images.githubusercontent.com/62640332/168007254-7db95912-d9a9-4944-a5ca-7be6fec4d90b.png)

디플로이먼트에 속한 파드가 삭제되면 레플리카셋이 확인하여 새로운 파드를 생성하여 설정한 replicas의 개수를 유지하지만,

디플로이에 속하지 않은 일반 파드 삭제시 어떤 컨트롤러도 이 파드를 관리 하지 안항서 그냥 삭제되고, 다시 생성X


```
# 디플로이먼트에 속한 파드는 상위 디플로이먼트를 삭제해야 파드가 삭제해야한다.
```

<br>
<br>


노드는 어떤 식으로 관리할까요? 우선 노드의 목적을 명확히 해야 합니다. 

노드는 `쿠버네티스 스케줄러에서 파드를 할당받고 처리하는 역할`을 합니다.

그런데 최근에 몇 차례 문제가 생긴 노드에 파드를 할당하면 문제가 생길 가능성이 높습니다. 

하지만 어쩔 수 없이 해당 노드를 사용해야 한다면 어떻게 할까요? 

이런 경우에는 영향도가 적은 파드를 할당해 일정 기간 사용하면서 모니터링해야 합니다. 

즉, 노드에 문제가 생기더라도 파드의 문제를 최소화해야 합니다. 

하지만 쿠버네티스는 모든 노드에 균등하게 파드를 할당하려고 합니다. 

그렇다면 어떻게 문제가 생길 가능성이 있는 노드라는 것을 쿠버네티스에 알려줄까요?

쿠버네티스에서는 이런 경우에 `cordon` 기능을 사용합니다.(해제 명령어는 `uncordon`)

![image](https://user-images.githubusercontent.com/62640332/168013662-f0fb55bc-e821-4608-afea-4f0a03c60ca3.png)

사용시 해당 노드의 상태가  `더이상 파드가 할당되지 않는 상태`로 변경. 파드수를 늘리든, 줄이든 cordon 설정한 노드는 변경되지 않음.

<br>
<br>

### ㅁ 노드의 커널을 업데이트하거나 노드의 메모리를 증설하는 등의 작업이 필요해서 노드를 꺼야 할 때는 어떻게 하면 좋을까요?

---

쿠버네티스를 사용하다 보면 정기 또는 비정기적인 유지보수를 위해 노드를 꺼야 하는 상황이 발생합니다. 

이런 경우를 대비해 쿠버네티스는 `drain` 기능을 제공합니다.

drain은 지정된 노드의 파드를 전부 다른 곳으로 이동시켜 해당 노드를 유지보수할 수 있게 합니다. 

drain은 실제로 파드를 옮기는 것이 아니라 노드에서 파드를 삭제하고 다른 곳에 다시 생성합니다. 

앞에서도 설명했지만 파드는 언제라도 삭제할 수 있기 때문에 쿠버네티스에서 대부분 이동은 파드를 지우고 다시 만드는 과정을 의미합니다. 

그런데 DaemonSet은 각 노드에 1개만 존재하는 파드라서 drain으로는 삭제할 수 없습니다.

```
# drain 명령과 ignore-daemonsets 옵션을 함께 사용합니다. 이 옵션을 사용하면 DaemonSet을 무시하고 진행
```

![image](https://user-images.githubusercontent.com/62640332/168028896-68a2e571-ffe5-4058-b68a-c926ea542f57.png)


![image](https://user-images.githubusercontent.com/62640332/168030734-afd386df-6951-4b89-abe0-2b304c1dc2a6.png)


<br>
<br>

### ㅁ 쿠버네티스 연결을 담당하는 서비스

---

일반적으로 서비스라고 하면 웹 서비스나 네트워크 서비스처럼 운영 체제에 속한 서비스 데몬 또는 개발 중인 서비스 등을 떠올릴 겁니다. 

그런데 쿠버네티스에서는 외부에서 쿠버네티스 클러스터에 접속하는 방법을 `서비스(service)`라고 합니다. 

서비스를 ‘소비를 위한 도움을 제공한다’는 관점으로 바라본다면 쿠버네티스가 외부에서 쿠버네티스 클러스터에 접속하기 위한 ‘서비스’를 제공한다고 볼 수 있습니다.


### ㅁ 노드포트

---

외부에서 쿠버네티스 클러스터의 내부에 접속하는 가장 쉬운 방법은 `노드포트(NodePort) 서비스`를 이용하는 것입니다. 

노드포트 서비스를 설정하면 모든 워커 노드의 특정 포트(노드포트)를 열고 여기로 오는 `모든 요청을 노드포트 서비스로 전달`합니다. 

그리고 노드포트 서비스는 해당 업무를 처리할 수 있는 `파드로 요청을 전달`합니다.

![image](https://user-images.githubusercontent.com/62640332/168031237-60b0163c-b62a-45a0-a3a5-3976c9bcdd61.png)

<br>

- 노드포트 자동으로 부하분산 되는 이유? 노드포트의 오브젝트 스펙에 적힌 np-pods와 디플로이먼트의 이름을 확인해 동일하면 같은 파드라고 간주하기 때문입니다

- 노드포트 생성방법] 오브젝트 스펙 파일, expose 명령어 사용

```
expose 명령어 실행시 포트번호 임의로 지정 불가능하며, 포트번호는 30000~32767에서 임의로 지정됨
```

### ㅁ 사용 목적별로 연결하는 인그레스

---

노드포트 서비스는 포트를 중복 사용할 수 없어서 1개의 노드포트에 1개의 디플로이먼트만 적용됩니다. 

그렇다면 여러 개의 디플로이먼트가 있을 때 그 수만큼 노드포트 서비스를 구동해야 할까요? 쿠버네티스에서는 이런 경우에 `인그레스`를 사용합니다. 

인그레스(Ingress)는 `고유한 주소를 제공`해 `사용 목적에 따라 다른 응답을 제공`할 수 있고, 트래픽에 대한 `L4/L7 로드밸런서`와 `보안 인증서`를 처리하는 기능을 제공합니다.

인그레스를 사용하려면 `인그레스 컨트롤러`가 필요합니다

여기서는 NGINX 인그레스 컨트롤러가 다음 단계로 작동합니다.

1. 사용자는 노드마다 설정된 노드포트를 통해 노드포트 서비스로 접속합니다. 이때 노드포트 서비스를 NGINX 인그레스 컨트롤러로 구성합니다.

2. NGINX 인그레스 컨트롤러는 사용자의 접속 경로에 따라 적합한 클러스터 IP 서비스로 경로를 제공합니다.

3. 클러스터 IP 서비스는 사용자를 해당 파드로 연결해 줍니다.

```
인그레스 컨트롤러는 파드와 직접 통신할 수 없어서 노드포트 또는 로드밸런서 서비스와 연동되어야 합니다. 따라서 노드포트로 이를 연동했습니다.
```

인그레스를 위한 설정 파일은 다음과 같습니다. 이 파일은 들어오는 주소 값과 포트에 따라 노출된 서비스를 연결하는 역할을 설정합니다.

- expose 명령으로 디플로이먼트(in-hname-pod, in-ip-pod)도 서비스로 노출합니다. 

외부와 통신하기 위해 클러스터 내부에서만 사용하는 파드를 클러스터 외부에 노출할 수 있는 구역으로 옮기는 것입니다. 

내부와 외부 네트워크를 분리해 관리하는 DMZ(DeMilitarized Zone, 비무장지대)와 유사한 기능입니다.

<br>
<br>

### ㅁ 클라우드에서 쉽게 구성 가능한 로드밸런서

---

앞에서 배운 연결 방식은 들어오는 요청을 모두 워커 노드의 노드포트를 통해 노드포트 서비스로 이동하고 이를 다시 쿠버네티스의 파드로 보내는 구조였습니다. 

이 방식은 매우 비효율적입니다. 그래서 쿠버네티스에서는 `로드밸런서(LoadBalancer)`라는 서비스 타입을 제공해 다음 그림과 같은 간단한 구조로 `파드를 외부에 노출하고 부하를 분산`합니다.

![image](https://user-images.githubusercontent.com/62640332/168427078-0a4c5ce2-ef19-40ad-bef4-02f028dd33b9.png)


- 왜 지금까지 로드밸런서를 사용하지 않았을까요? 

: 로드밸런서를 사용하려면 로드밸런서를 이미 구현해 둔 서비스업체의 도움을 받아 쿠버네티스 클러스터 외부에 구현해야 하기 때문입니다. 

클라우드에서 제공하는 쿠버네티스를 사용하고 있다면 다음과 같이 선언만 하면 됩니다.

그러면 쿠버네티스 클러스터에 로드밸런서 서비스가 생성돼 외부와 통신할 수 있는 IP(EXTERNAL-IP)가 부여되고 외부와 통신할 수 있으며 부하도 분산됩니다.

<br>

### ㅁ MetalLB

---

: 온프레미스에서 로드밸런서를 사용하려면 내부에 로드밸런서 서비스를 받아주는 구성이 필요한데, 이를 지원하는 것이 `MetalLB`입니다. 

MetalLB는 베어메탈(bare metal, 운영 체제가 설치되지 않은 하드웨어)로 구성된 쿠버네티스에서도 로드밸런서를 사용할 수 있게 고안된 프로젝트입니다. 

MetalLB는 특별한 네트워크 설정이나 구성이 있는 것이 아니라 기존의 L2 네트워크(ARP/NDP)와 L3 네트워크(BGP)로 로드밸런서를 구현합니다.

![image](https://user-images.githubusercontent.com/62640332/168427259-10ec0bee-c52d-4295-8f5a-21a5d1c44f3c.png)

MetalLB 컨트롤러는 작동 방식(Protocol, 프로토콜)을 정의하고 EXTERNAL-IP를 부여해 관리합니다. 

MetalLB 스피커(speaker)는 정해진 작동 방식(L2/ARP, L3/BGP)에 따라 경로를 만들 수 있도록 네트워크 정보를 광고하고 수집해 각 파드의 경로를 제공합니다. 

이때 L2는 스피커 중에서 리더를 선출해 경로 제공을 총괄하게 합니다.

<br>
<br>

### ㅁ HPA

---

부하량에 따라 디플로이먼트의 파드 수를 유동적으로 관리하는 기능을 제공합니다. 이를 HPA(Horizontal Pod Autoscaler)라고 합니다.

<br>
<br>

### ㅁ 데몬셋

---

데몬셋은 디플로이먼트의 replicas가 노드 수만큼 정해져 있는 형태라고 할 수 있는데, 노드 하나당 파드 한 개만을 생성합니다.

데몬셋은 언제 사용할까요? 사실 데몬셋은 이미 여러 번 사용했습니다. 

Calico 네트워크 플러그인과 kube-proxy를 생성할 때 사용했고, MetalLB의 스피커에서도 사용했습니다. 

이들의 공통점은 노드의 단일 접속 지점으로 노드 외부와 통신하는 것입니다. 따라서 파드가 1개 이상 필요하지 않습니다. 

결국 노드를 관리하는 파드라면 데몬셋으로 만드는 게 가장 효율적입니다.

<br>
<br>

### ㅁ 컨피그 맵

---

설정(config)을 목적으로 사용하는 오브젝트입니다. MetalLB를 구성할 때 컨피그맵을 사용해 봤습니다. 

인그레스에서는 설정을 위해 오브젝트를 인그레스로 선언했는데, 왜 MetalLB에서는 컨피그맵을 사용했을까요? 

명확하게 규정하기는 어려운데 인그레스는 오브젝트가 인그레스로 지정돼 있지만, 

MetalLB는 프로젝트 타입으로 정해진 오브젝트가 없어서 범용 설정으로 사용되는 컨피그맵을 지정했습니다.

<br>
<br>

### ㅁ PV와 PVC

---

이제 파드가 언제라도 생성되고 지워진다는 것을 충분히 알았을 것입니다. 쿠버네티스에서 의도적으로 이렇게 구현했습니다. 

그런데 때때로 파드에서 생성한 내용을 기록하고 보관하거나 모든 파드가 동일한 설정 값을 유지하고 관리하기 위해 공유된 볼륨으로부터 공통된 설정을 가지고 올 수 있도록 설계해야 할 때도 있습니다.


- 임시: emptyDir

- 로컬: host Path, local

- 원격: persistentVolumeClaim, cephfs, cinder, csi, fc(fibre channel), flexVolume, flocker, glusterfs, iscsi, nfs, portworxVolume, quobyte, rbd, scaleIO, storageos, vsphereVolume

- 특수 목적: downwardAPI, configMap, secret, azureFile, projected

- 클라우드: awsElasticBlockStore, azureDisk, gcePersistentDisk


쿠버네티스는 필요할 때 `PVC(PersistentVolumeClaim, 지속적으로 사용 가능한 볼륨 요청)`를 요청해 사용합니다. 

PVC를 사용하려면 `PV(PersistentVolume, 지속적으로 사용 가능한 볼륨)`로 볼륨을 선언해야 합니다. 간단하게 PV는 볼륨을 사용할 수 있게 준비하는 단계이고, PVC는 준비된 볼륨에서 일정 공간을 할당받는 것입니다. 

<br>

ㅁ PV로 볼륨을 선언할 수 있는 타입

![image](https://user-images.githubusercontent.com/62640332/168436459-bdb91b50-f0c6-4f30-a61d-60788c864e96.png)


PVC는 PV와 구성이 거의 동일합니다.

- PVC는 사용자(개발자)간 볼륨을 요청하는 데 사용
- PV는 사용자가 요청할 볼륨 공간을 관리자가 만듬

![image](https://user-images.githubusercontent.com/62640332/168439036-6ddc59b4-29b4-425a-a1d5-7881ca3a9810.png)

 PV와 PVC를 구성해서 PV와 PVC를 구성하는 주체가 관리자와 사용자로 나뉜다는 것을 확인했습니다. 
 
 또한 관리자와 사용자가 나뉘어 있지 않다면 굳이 PV와 PVC를 통하지 않고 바로 파드에 공유가 가능한 NFS 볼륨을 마운트할 수 있음을 알았습니다.

<br>
<br>

### ㅁ 스테이트풀셋

---

지금까지는 파드가 replicas에 선언된 만큼 무작위로 생성될 뿐이었습니다. 그런데 파드가 만들어지는 이름과 순서를 예측해야 할 때가 있습니다. 

주로 레디스(Redis), 주키퍼(Zookeeper), 카산드라(Cassandra), 몽고DB(MongoDB) 등의 마스터-슬레이브 구조 시스템에서 필요합니다.

`스테이트풀셋(StatefulSet)`을 사용합니다. 

스테이트풀셋은 volumeClaimTemplates 기능을 사용해 PVC를 자동으로 생성할 수 있고, 각 파드가 순서대로 생성되기 때문에 `고정된 이름, 볼륨, 설정 등`을 가질 수 있습니다. 

그래서 StatefulSet(이전 상태를 기억하는 세트)이라는 이름을 사용합니다. 다만, 효율성 면에서 좋은 구조가 아니므로 요구 사항에 맞게 적절히 사용하는 것이 좋습니다.

```
statefulset은 expose를 지원하지 않기떄문에, 로드밸런서 서비스를 실행해야합니다.

expose 명령으로 서비스를 생성할수 있는 오브젝트는 디플로이먼트, 파드, 레플리카셋, 레플리케이션 컨트롤러입니다.
```

- 스테이트풀셋은 헤드리스(Headless) 서비스로 노출한다고 하던데요?
    
네, 일반적으로는 맞습니다. 헤드리스 서비스는 `IP를 가지지 않는 서비스 타입`으로 중요한 자원인 IP를 절약할 수 있을 뿐만 아니라, 

스테이트풀셋과 같은 상태를 가지고 있는 오브젝트를 모두 노출하지 않고 상태 값을 외부에 알리고 싶은 것만 `선택적으로 노출`하게 할 수 있습니다. 

따라서 일반적으로는 스테이트풀셋은 헤드리스 서비스로 노출하나, IT에는 정답이 없듯 고정된 이름을 사용하면서 외부에 모든 스테이트풀셋을 노출하고자 하는 경우에는 노드포트나 로드밸런서 서비스로 노출할 수 있습니다. 

현재의 구성에서 헤드리스 서비스로 노출하고자 하는 경우에는 다음 코드를 사용해 노출할 수 있습니다.

노출된 IP는 없지만 내부적으로 각 파드의 이름과 노출된 서비스 이름등을 조합한 도메인 이름으로 아래와 같이 쿠버네티스 클러스터 내에서 통신할 수 있는 상태가 됩니다. 

이를 가능하게 해주는 CoreDNS는 6장 ‘쿠버네티스 내에서 도메인 이름을 제공하는 CoreDNS’를 참조하기 바랍니다.


## ㅁ 도커

---

파드들은 워커 노드라는 노드 단위로 관리하며, 

워커 노드와 마스터 노드가 모여 쿠버네티스 클러스터가 됩니다. 

그리고 파드는 1개 이상의 컨테이너로 이루어져 있습니다.

파드는 쿠버네티스로부터 IP를 받아 컨테이너가 외부와 통신할 수 있는 경로를 제공합니다. 

그리고 컨테이너들이 정상적으로 작동하는지 확인하고 네트워크나 저장 공간을 서로 공유하게 합니다. 

파드가 이러한 환경을 만들기 때문에 컨테이너들은 마치 하나의 호스트에 존재하는 것처럼 작동할 수 있습니다. 

정리하면, 컨테이너를 돌보는 것이 파드고, 

파드를 돌보는 것이 쿠버네티스 워커 노드이며, 

워커 노드를 돌보는 것이 쿠버네티스 마스터입니다. 

그런데 쿠버네티스 마스터 역시 파드(컨테이너)로 이루어져 있습니다.

![image](https://user-images.githubusercontent.com/62640332/168440490-8e24dc77-90e5-4fbf-996e-8801806d6f61.png)

이 구조를 이루는 가장 기본인 컨테이너는 `하나의 운영 체제` 안에서 `커널을 공유`하며 `개별적인 실행 환경을 제공`하는 격리된 공간입니다. 

여기서 개별적인 실행 환경이란 CPU, 네트워크, 메모리와 같은 시스템 자원을 독자적으로 사용하도록 할당된 환경을 말합니다. 

개별적인 실행 환경에서는 실행되는 프로세스를 구분하는 ID도 컨테이너 안에 격리돼 관리됩니다. 

그래서 각 컨테이너 내부에서 실행되는 애플리케이션들은 서로 영향을 미치지 않고 독립적으로 작동할 수 있습니다.

![image](https://user-images.githubusercontent.com/62640332/168440511-8d13d7b4-becb-47ab-a339-6cc8fade9c52.png)

각 컨테이너가 독립적으로 작동하기 때문에 여러 컨테이너를 효과적으로 다룰 방법이 필요해졌습니다. 

오래전부터 유닉스나 리눅스는 하나의 호스트 운영 체제 안에서 자원을 분리해 할당하고, 실행되는 프로세스를 격리해서 관리하는 방법을 제공했습니다. 

하지만 파일 시스템을 설정하고 자원과 공간을 관리하는 등의 복잡한 과정을 직접 수행해야 해서 일부 전문가만 사용할 수 있다는 단점이 있었습니다. 

이런 복잡한 과정을 쉽게 만들어 주는 도구로 등장한 것이 `도커`입니다. 

도커는 `컨테이너를 사용하는 방법을 명령어로 정리한 것`이라고 보면 됩니다. 

도커를 사용하면 사용자가 따로 신경 쓰지 않아도 컨테이너를 생성할 때 개별적인 실행 환경을 분리하고 자원을 할당합니다.

![image](https://user-images.githubusercontent.com/62640332/168440558-1d242c73-ed58-4501-a877-eea07bb00630.png)

쿠버네티스 인프라를 구성하는 데 가장 적합한 도구로 도커를 선택

<br>

베이그런트 이미지는 이미지 자체로는 사용할 수 없고 베이그런트를 실행할 때 추가해야만 사용할 수 있습니다. 

이와 마찬가지로 컨테이너 이미지도 그대로는 사용할 수 없고 도커와 같은 CRI로 불러들여야 컨테이너가 실제로 작동합니다. 

이는 실행 파일과 실행된 파일 관계로 볼 수 있습니다. 

따라서 컨테이너를 삭제할 때는 내려받은 이미지와 이미 실행된 컨테이너를 모두 삭제해야만 디스크의 용량을 온전히 확보할 수 있습니다.

1. 이미지 찾기
2. 실행하기
3. 디렉터리와 연결하기
4. 삭제하기


### 1. 이미지 검색

---

이미지는 레지스트리(registry)라고 하는 저장소에 모여 있습니다. 

레지스트리는 도커 허브(https://hub.docker.com)처럼 공개된 유명 레지스트리일 수도 있고, 내부에 구축한 레지스트리일 수도 있습니다. 

```
이미지 찾기
docker search <검색어>
```

![image](https://user-images.githubusercontent.com/62640332/168469659-7009be19-8f04-42de-a919-c7b0981bb170.png)

• INDEX: 이미지가 저장된 레지스트리의 이름입니다.

• NAME: 검색된 이미지 이름입니다. 공식 이미지를 제외한 나머지는 ‘레지스트리 주소/저장소 소유자/이미지 이름’ 형태입니다.

• DESCRIPTION: 이미지에 대한 설명입니다.

• STARS: 해당 이미지를 내려받은 사용자에게 받은 평가 횟수입니다. 사용자가 좋은 평가를 주고 싶을 때 스타(STAR)를 추가합니다. 숫자가 클수록 신뢰성 높은 이미지일 수 있습니다.

• OFFICIAL: [OK] 표시는 해당 이미지에 포함된 애플리케이션, 미들웨어 등을 개발한 업체에서 공식적으로 제공한 이미지라는 의미입니다.

• AUTOMATED: [OK] 표시는 도커 허브에서 자체적으로 제공하는 이미지 빌드 자동화 기능을 활용해 생성한 이미지를 의미합니다.

<br>

```
이미지 다운
docker pull <검색어>
```

<br>

![image](https://user-images.githubusercontent.com/62640332/168469735-97608036-7600-44ae-b99c-4f06970d6b3f.png)

• 태그(tag): Using default tag와 함께 뒤에 따라오는 태그 이름을 통해 이미지를 내려받을 때 사용한 태그를 알 수 있습니다. 

아무런 조건을 주지 않고 이미지 이름만으로 pull을 수행하면 기본으로 latest 태그가 적용됩니다. 

latest 태그는 가장 최신 이미지를 의미합니다. 따라서 내려받는 이미지 버전이 다를 수 있습니다.

<br>

• 레이어(layer): d121f8d1c412, ebd81fc8c071, 655316c160af, d15953c0e0f8, 2ee525c5c3cc는 pull을 수행해 내려받은 레이어입니다. 

하나의 이미지는 여러 개의 레이어로 이루어져 있어서 레이어마다 Pull complete 메시지가 발생합니다.

<br>

• 다이제스트(digest): 이미지의 고유 식별자로, 이미지에 포함된 내용과 이미지의 생성 환경을 식별할 수 있습니다. 

식별자는 해시(hash) 함수로 생성되며 이미지가 동일한지 검증하는 데 사용합니다. 이름이나 태그는 이미지를 생성할 때 임의로 지정하므로 이름이나 태그가 같다고 해서 같은 이미지라고 할 수 없습니다. 

그러나 다이제스트는 고유한 값이므로 다이제스트가 같은 이미지는 이름이나 태그가 다르더라도 같은 이미지입니다.

<br>

• 상태(Status): 이미지를 내려받은 레지스트리, 이미지, 태그 등의 상태 정보를 확인할 수 있습니다. 

형식은 ‘레지스트리 이름/이미지 이름:태그’입니다. 여기서는 내려받은 이미지는 docker.io 레지스트리에서 왔으며, 

이미지의 이름은 nginx, 태그는 앞서 설명한 것처럼 별도의 태그를 지정하지 않았기 때문에 기본 태그인 latest입니다.

<br>
<br>

#### ㅁ 이미지 태그


태그는 이름이 동일한 이미지에 추가하는 식별자입니다. 

이름이 동일해도 도커 이미지의 버전이나 플랫폼(CPU 종류나 기본 베이스를 이루는 운영 체제 등)이 다를 수 있기 때문에 이를 구분하는 데 사용합니다. 

이미지를 내려받거나 이미지를 기반으로 컨테이너를 구동할 때는 이미지 이름만 사용하고 태그를 명시하지 않으면 latest 태그를 기본으로 사용합니다. 

이미지 태그와 관련된 정보는 해당 이미지의 도커 허브 메뉴 중 Tags 탭에서 확인할 수 있습니다.

<br>
<br>

#### ㅁ 이미지의 레이어 구조


앞에서 컨테이너 이미지는 실행 파일이라고 했는데, 사실 이미지는 애플리케이션과 각종 파일을 담고 있다는 점에서 ZIP 같은 압축 파일에 더 가깝습니다.

그런데 압축 파일은 압축한 파일의 개수에 따라 전체 용량이 증가합니다. 하지만 이미지는 같은 내용일 경우 여러 이미지에 동일한 레이어를 공유하므로 전체 용량이 감소합니다.

다음 그림은 내부에 동일한 파일이 포함된 압축 파일과 이미지를 보여줍니다. 압축 파일은 내용이 같은 파일 두 개가 각 압축 파일에서 공간을 독립적으로 점유합니다. 

그에 반해 이미지는 내용이 같은 레이어1, 레이어2를 공유하기 때문에 전체 공간에서 봤을 때 상대적으로 용량을 적게 차지합니다.

![image](https://user-images.githubusercontent.com/62640332/168470277-c4430b0f-8ccd-47b3-bee8-0a72be448bc1.png)

```
다운로드한 이미지 조회
docker images <이미지 이름>
```

![image](https://user-images.githubusercontent.com/62640332/168470821-27c5a5ea-d75e-4ca8-8e63-50fbcc67a015.png)

stable과 latest 이미지의 크기는 각각 132MB, 133MB이지만, 실제로는 69.2MB에 해당하는 레이어를 두 이미지가 공유하고 있습니다

<br>
<br>

### 2. 컨테이너 실행

---

docker run으로 컨테이너를 생성하면 결괏값으로 16진수 문자열이 나옵니다. 

이 문자열은 컨테이너를 식별할 수 있는 고유한 `ID`

컨테이너를 생성하게 되면 마스터 노드 내부에 존재하게 된다. 

그리고 curl 127.0.0.1 명령으로 웹 페이지 정보를 가져 올려고 시도하면 Connection refuesed 오류가 발생한다.

이유는 로컬 호스트(127.0.0.1)의 지정한 포트로 전달만 될 뿐 컨테이너까지 도달하지 못해서 이다.

호스트에 도달한 후 컨테이너로 도달하기 위한 추가 경로가 설정 필요하다.

![image](https://user-images.githubusercontent.com/62640332/168471382-3a3ea4a9-9395-4c02-9bf2-b2c46559808f.png)

```
컨테이너는 변경 불가능한 인프라(immutable infrastructure)를 지향합니다. 
변경 불가능한 인프라는 초기에 인프라를 구성하면 임의로 디렉터리 연결이나 포트 노출과 같은 설정을 변경할 수 없습니다. 
따라서 컨테이너에 적용된 설정을 변경하려면 새로운 컨테이너를 생성해야 합니다. 
이러한 특성 덕분에 컨테이너로 배포된 인프라는 배포된 상태를 유지한다는 장점이 있습니다.
```

```
- 위의 에러가뜬 컨테이너 실행 방법
[root@m-k8s ~]# docker run -d --restart always nginx

- 추가로 경로를 설정해 정상적으로 컨테이너 실행 방법
[root@m-k8s ~]# docker run -d -p 8080:80 --name nginx-exposed --restart always nginx
```

<br>
<br>

### 3. 디렉토리와 연결하기

---

![image](https://user-images.githubusercontent.com/62640332/168472006-9454454f-c550-402b-b1df-92cb60ced0bf.png)

![image](https://user-images.githubusercontent.com/62640332/168472111-251ee1c2-f855-4df9-9d89-7a2e2996ba01.png)

<br>
<br>

### 4. 컨테이너 삭제 및 이미지 삭제

---

- 볼륨(volume)은 도커가 직접 관리하며 컨테이너에 제공하는 호스트의 공간

- 컨테이너나 이미지를 삭제하기 전에 먼저 컨테이너를 정지해야 합니다. 
  
  삭제할 때 말고도 동일한 호스트의 포트를 사용하는 컨테이너를 배포하거나 작동 중인 컨테이너의 사용 자체를 종료할 때도 먼저 컨테이너를 정지해야 합니다.

```
- 컨테이너 정지 명령
docker stop <컨테이너 이름 | ID>

- 컨테이너 삭제 명령
docker rm <컨테이너 이름 | ID>

- 이미지 삭제 명령
docker rmi <이미지 이름 | ID>
```

<br>
<br>

### ㅁ 컨테이너 이미지 만들기

---

컨테이너 이미지 만드는 방법

1. 기본적인 빌드
2. 용량 줄이기
3. 컨테이너 내부 빌드
4. 멀티 스테이지


#### 1. 기본 방법으로 빌드하기

컨테이너 빌드 과정

![image](https://user-images.githubusercontent.com/62640332/168473881-fd271f0b-6218-4ab9-b42c-a86208113619.png)

```
컨테이너 이미지 빌드 명령어
docker build <생성할 파일이름> <생성할 파일위치>
```

- 목적지에 따라 출발지 표시가 다른 이유
    

현재 구동 중인 호스트의 가상 인터페이스 IP(192.168.1.10)와 로컬호스트 IP(127.0.0.1)의 60431번 포트에 요청을 보내면 출발지의 IP가 다름을 확인할 수 있습니다.

```
[root@m-k8s 4.3.1]# curl 192.168.1.10:60431
src: 192.168.1.10 / dest: 192.168.1.10
[root@m-k8s 4.3.1]# curl 127.0.0.1:60431
src: 172.17.0.1 / dest: 127.0.0.1
```

현재 컨테이너는 외부 요청이 목적지에 도착하기 전에 거친 네트워크 인터페이스의 IP와 포트를 출발지(src)로 표시하게 작성됐습니다. 

그런데 호스트 인터페이스(eth1)의 IP가 192.168.1.10이고, 컨테이너 브리지 인터페이스(docker0)의 IP는 172.17.0.1입니다. 

eth1은 외부 요청을 받아들이는 네트워크 인터페이스이고, docker0는 도커 컨테이너가 사용하는 네트워크 인터페이스입니다. 

따라서 도커 컨테이너가 외부와 통신하려면 docker0를 거쳐야 합니다.

![image](https://user-images.githubusercontent.com/62640332/168474256-b9b686c7-0f23-4295-85d6-53f8342f7500.png)


도커 컨테이너는 생성될 때 docker0에 부여된 172.17.0.0/16 범위에 해당하는 172.17.0.2~172.17.255.254(172.17.0.1은 docker0에서 사용) 사이의 IP를 할당받습니다. 

따라서 현재 생성된 컨테이너는 172.17.0.0/16에 속하는 IP를 가진 상태입니다. 

이 컨테이너는 모든 네트워크 어댑터(0.0.0.0)의 60431번 포트로 들어오는 요청을 컨테이너 내부로 전달하도록 옵션으로 설정했습니다. 

이 옵션을 설정하면 외부 IP에서 들어오는 요청을 컨테이너 내부 IP로 전달하는 경로 전달 규칙이 리눅스 호스트에 설정됩니다. 

그 결과 컨테이너 내부로 요청을 보낼 경우 출발지는 앞에서 실행한 첫 번째 명령의 수행 결과와 같이 192.168.1.10:60431이 표시됩니다.

반면에 127.0.0.1이나 localhost와 같은 내부 IP에서 컨테이너 내부로 요청을 전달할 경우 docker-proxy 프로세스가 컨테이너 내부로 요청을 전달하는 역할을 하며 

이 과정에서 도커가 사용하는 네트워크 인터페이스의 IP인 172.17.0.1을 출발지로 사용합니다. 

따라서 docker-proxy 프로세스에 문제가 발생하면 내부 IP로는 컨테이너 내부에 요청을 전달할 수 없을 뿐만 아니라 컨테이너 내부에서 호스트 외부 IP와 포트로 요청을 보낼 수 없는 문제가 발생합니다. 

자세한 내용은 이 책의 범위를 넘어가므로 더 설명하지 않습니다. 궁금하신 분은 인터넷에서 헤어핀(Hairpin) NAT에 관해 찾아보기 바랍니다.

<br>
<br>

#### 2. 콘테이너 용량 줄이기

![image](https://user-images.githubusercontent.com/62640332/168474616-59babac5-1876-4a1f-809b-c7bc9f876b22.png)

openjdk 사용 안하고 GCR에서 제공하는 distroless로 변경

<br>

#### 3. 컨테이너 내부에서 컨테이너 빌드하기

![image](https://user-images.githubusercontent.com/62640332/168478972-4fb07073-93c6-4ddd-8360-8a93adc2641a.png)

빌드 과정 자체를 openjdk 이미지에서 진행하므로 Dockerfile만 필요

새로 생성된 nohost-img가 618MB로 이미지 중에서 가장 용량이 큽니다. 

nohost-img는 컨테이너 내부에서 빌드를 진행하기 때문에 빌드 중간에 생성한 파일들과 내려받은 라이브러리 캐시들이 최종 이미지인 nohost-img에 그대로 남습니다. 

따라서 빌드 최종 결과물만 전달했던 basic-img보다 더 커지게 됩니다.

컨테이너 이미지는 커지면 커질수록 비효율적으로 작동할 수밖에 없습니다. 따라서 openjdk로 컨테이너 내부에서 컨테이너를 빌드하는 것을 좋지 않은 방법입니다.

<br>

#### 4. 최적화해 컨테이너 빌드하기

멀티 스테이지 빌드(Multi-Stage Build, 이후 멀티 스테이지라고 표현함) 방법은 최종 이미지의 용량을 줄일 수 있고 호스트에 어떠한 빌드 도구도 설치할 필요가 없습니다.

![image](https://user-images.githubusercontent.com/62640332/168479244-d968508b-3dc0-4b44-b17c-1c27b2099471.png)

멀티 스테이지의 핵심은 빌드하는 위치와 최종 이미지를 `분리`하는 것입니다. 

그래서 최종 이미지는 빌드된 JAR을 가지고 있지만, 용량은 줄일 수 있습니다.