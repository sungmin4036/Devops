ㅁ What is Legacy Application?

: 오래되고 구식의 아키텍처, 프레임워크, 플랫폼, 프로그래밍 언어로 구성된 시스템 하드ㅞ어 통칭

- Risk

    - 새로운 기술이나 비즈니스 요구사항을 수용하기 힘듦
    - 유지보수 비용이 높음

레거시는 건드리기 쉽고, 잘못 건드리면 아주큰 문제가 발생할수 있다.

그러나 반대로 그만큼 비즈니스에 중요한 서비스인데 기능 개발이나 유지보수가 어렵다면 팀의 속도가 느려지고

잠재적으로 더큰 비용을 소용할것입니다.

이러한 레거시 애플리케이션 현대화는 그 변화정도를 얼마나 어떻게 가져가느냐에 따라 비용과 리스크가 굉장히 상이하다

따라서 제품 로드맵이나 사업계획에 따라 레거시의 일부만 개선할지, 기능을 분리해서 새로운 플랫폼으로 이식할지

혹은 아예 새로 바닥부터 개발할지 선택해서 실행해야 합니다.


![image](https://user-images.githubusercontent.com/62640332/155560215-b0b1d3e7-05fb-493f-85e5-4a78d74e3e1a.png)

버즈빌은 버즈빌의 자체 앱과, B2B 고객사들의 앱에 광고 SDK 형태로 서비스를하고 이씨때문에

앱과 SDk의 프론트 서비스, 그리고 모노리식한 광고서버로 나위어 구성되어있었습니다.

운영환경은 처음부터 AWS에서 클라우드 기반으로 운영중이였습니다.

![image](https://user-images.githubusercontent.com/62640332/155563569-3c1f7b9c-bcaa-413e-aebe-415ec00411b3.png)

광고 도메인은 굉장히 복잡한데, 광고주들의 다양한 디멘드를 모아서 목적에 따라 광고를 세팅하고 성과를 확인하며

실시간으로 광고 요청에대해 사용자에 최적화된 광고를 추천합니다.

그리고 예산을 균등하게 소진할수 있도록 실시간으로 제어합니다.

그리고 퍼블리셔의 지면 수익을 극대화 하기위해 버즈빌이 수급한 디맨드가 아닌 다양한 네트워크 광고를 연동하고,

사용자들에게 광고 외에도 여러 읽을거리를 제공하기위해 컨테츠를 수집하고 노출하고 있습니다.

요지는 소수의 모노리틱한 서비스에 다양한 stakehloder들의 요구사항들이 매우 복잡하게 얽혀있다.

다양한 기능들이 소수의 monolith에 집중되다 보니 각 모듈들 간의 인터페이스가 명확하게 분리되지 않아

얽히고 섥혀있어고, 점점 서비스 개발과 배포에 부담이 늘어 났습니다.

배포시간이 오래걸리고 특정 모듈의 버그가 전체 서비스의 장애로 이어지는 경우도 발생했습니다.

이런 큰 오류가 발생하면 모든 기능의 배포가 중단되어야 했습니다.

<br>
<br>
<br>
---

AWS 마이크로서비스 소개 페이지를 가보면, 마이크로 서비스는  소프트웨어가 잘 정의된  API를 통해  통신하는 작은

독립적인 서비스로 구성된 소프트웨어 개발을 위한 아키텍처와  조직적인 접근 방식이라 정의

버즈빌에서는 기능조직에서 목적 조직으로 팀구조를 변경하면서 서비스 아키텍처를 어떻게 가져갈지 고민을 하였고, 마이크로서비스와 API를

잘 정의하는 것이 주된 방향이었기 때문 입니다.


![image](https://user-images.githubusercontent.com/62640332/155564106-f4fa2506-6e06-48e0-83b2-a8e2727b9d3f.png)

MSA 단위는 도메인 주도 개발을 채택, 바운디드 컨텍스트에 대해 고민했습니다.

쿠버네틱스 와 서비스 매쉬를 이용하여 마이크로 서비스간 인증, 로드밸런싱, 복잡한 트래픽 규칙등을 관리하고자 했습니다.

서비스간 통신은 기존에는 REST API 방식을 활용했으나 MSA 도입 이후로는 gRPC를 적극적으로 활용

분산 시스템에서는 내부 메소드 호출이 아닌 네트워크 홉이 추가되는 것이 가장큰 부담인데 gRPC는 바이너리 프로토콜 이여서 성능적으로 

훌륭하고 인터페이스 정의 언어인 protobuf를 통해 RPC를 미리 정의 할수 있다느 것이 가장큰 장점

소프트웨어 개발주기의 경우 모놀리식의 경우 기능 추가하는 것이 모듈 하나 추가하는 정도에 불가하지만

MSA에서는 기본적인 프로젝트 레이아웃, CI, 로깅 등을 매번 바닥부터 개발해야 하는경우가 빈번하게 발생

이 과정이 한 두 스프린트 내에  빠르게 진행될 수 있도록 표준 절차를 정의

먼저 디자인 캔버스나 문서로 서비스 요구사항을 정의하고 피드백을 받습니다.

그다음 protobuf로 API 정의를 작성해서 팀간 협의를 마칩니다.

그러면 CI에 의해 gRPC 서버와 클라이언트 코드들이 생성되는데 이러한 generated code가 포함된 프로젝트 뼈대 생성은

사내에서 권장하는 설정들이 모두 포함된 템플릿 프로젝트들을 이용합니다.

그러면 제품팀은 핵심 비즈니스 로직만 구현하면 배포준비하면 되는것이죠.

서비스에 종속된 데이터베이스 등은 테라폼을 PR기반으로 자동화하여 제품팀에서 필요한 RDS나 DynamoDB 등을 추가하여 요청을 보내면

DevOps 팀이 리뷰 후 승인하면 생성되도록 구현해, 서비스를 소유한 제품팀이 서비스 생성부터 배포까지 직접 진행할수 있게 하였습니다.

인프라적으로 DevOps나 Data Engineering 조직이 준비해야 하는  이니셔티브도 많지만, 개발팀 뿐 아니라 제품팀 

전체 그리고 나아가 비지니스 조직과도 이러한 방형성이 잘 정렬되는것이 중요합니다.

![image](https://user-images.githubusercontent.com/62640332/155565790-9b4c8aca-9906-4c71-848d-e04f00b56c02.png)

모노리스에서 담당하던 복잡하고 다양한 도메닌을 관심사별로 분리합니다.

![image](https://user-images.githubusercontent.com/62640332/155565916-5e905b34-ddd4-475d-b8fc-645f9f96bd5c.png)

철저히 모델 기준으로 bounded context를 나눠 볼수도 있고, 제품 개발을 담당하는 팀으로도 나눌수 있습니다.

각 컴포넌트는 독립적으로 배포되고, 확장이 가능해야 합니다.

ex) 광고 추천을 위해 타사로부터 사용자 이벤트를 받는 모듈이 있는데 자사 트래픽과 상관없이 이벤트가 폭증할 수 있습니다.

이벤트 수집 기능이 마이크로서비스로 개발되어 분리된 경우 전체 서비스가 아닌 이벤트 수집 서비스만 확장할수 있게 됩니다.

특정 모듈의 장애가 전체로 전파되는 문제도 방지할 수 있습니다.

ex) 외부 광고 네트워크를 연동하는 서비스가 장애로 일시적으로 동작안해도 폭파반경을 네트워크 광고로 한정하고,

직접 운영하는 광고하느 서버는 동작하여, 광고를 계속해서 송출하도록 할 수 있습니다.

![image](https://user-images.githubusercontent.com/62640332/155566349-4cb04d89-2137-4112-b0e6-75db9d711c6d.png)

이러한 마이크로서비스를 어떤 배포 모델을 가져갈지도 고민했습니다.

기존에는 ALB를 통해 클라이언트 요청을 유입받고 각 프론트 서비스 들이 배포되어 있는

EC2 인스턴스들로 로드밸런싱을 하였으며, 내부적으로 호출되는 서비스들에 대해서도 VPC 내부 서비스 디스커버리를 위한 Internal ALB를 구성하였습니다.

![image](https://user-images.githubusercontent.com/62640332/155567130-3343eb3b-b421-4a4d-b78e-ae6c61237b64.png)

부하에 대한 반응은 CloudWatch로 CPU자원을 모니터링 하여 Auto Scaling Group을 scale out/in을 하였습니다.

이 모델 에서는 독립적으로 배포되는 마이크로서비스가 추가될 떄마다 서비스 디스커버리,  오토스케일링, 

로드밸런싱에 대한 오버헤드가 늘어나는 구조입니다.

그리고 서비스별로  요구하는 리소스가 상이할수 있기 떄문에 서비스별로 각기 다른 Launch Configuration을 관리하여

필요데 따라 인스턴스 타입, EBS 볼륨들을 정의하고, 변경하는 것도 가능합니다만 이러한 인프라 설정을 일일이 

관리하는것은 상당히 번거로운 일 

![image](https://user-images.githubusercontent.com/62640332/155567189-7e0665dc-237f-432c-a113-fe18fefbfe96.png)

이런 문제 해결하기 위해 MSA를 컨테이너로 배포하고, 여러 노드에 배포된 컨테이너에 대해서 오케스트레이션을 해주는

도구에 대한 요구사항이 있었고,  이미 많은 조직에서 활용하는 것처럼 버즈빌도 쿠버네티스 도입

쿠버네티스를 활용하면 프로덕션 규모의 대규모 클러스터에서 손쉽게 마이크로서비스를 배포하고, 네트워킹 및 확장을 할수 있습니다.

서비스 리소를 통해 여러 노드에 배포되어있는  컨테이너들의 IP 주소나 포트등을 추상화 합니다.

외부요청은 Ingresss를 통해 ALB로 부터 유입되는 트랙픽을 원하는 서비스로 유입시킬수 있을뿐 아니라

내부 서비스들에 대한 디스커버리나 로드밸런싱 또한 서비스를 통해 이루어 집니다.

Deployment나 RepicaSet을 이용해  손쉽게 신규 버전을 배포할 수 있으며, CPU, Memory, RPS등에 기반한 횡적 확장이 간편합니다.

리소스를 많이 필요로 하는 워크로드의 경우 간단하게 필요로 하는 CPU, 메모리 등의 자원만 명시해주면 

여유공간이 있는 노드에 알아서 스케쥴링 되고 서비스 EndPoint IP 목록에 추가되어 요청을 처리하기 시작

이와같이 컨테이너화만 한다면 클러스터에 새로운 마이크로서비스 배포하는 일이 매우 간단해 집니다.

![image](https://user-images.githubusercontent.com/62640332/155567970-1118816e-af88-4caf-b40f-1640890cb535.png)

이러한 쿠버네틱스도 공짜는 아닙니다. 그자체로 복잡한 오케스트레이션 도구이다보니 클러스터 생성 및 관리에 대한 운영코스트 존재합니다.

특히 Control Plane의 고가용성 설정이나 쿠버네티스의 모든 클러스터 설정이 저장되는 etcd같은 스토어에 대한 

관리, 그리고 Pod Networking을 위한 CNI, DNS 등에 대한 운영방식이 수반하는데요.

버즈빌에서는 초반에 몇몇 프러덕션 워크로드를  kops라는 쿠버네티스 클러스터 관리도구를 활용하여 운영해오고 있었습니다.

커뮤니티에서 충분히 잘 지원되는 프로젝트 였지만 운영에 은근히 리소스가 많이 투입되었습니다.

특히 버전 업그레이드마다 마스터 그룹 롤링 시 크고 작은 이슈가 발생했고, etcd major 버전이 2 ->  3 으로 올가는 등의

변화가 있을 때엔 주의가 필요했습니다. 그런데 떄마침 EKS가 GA 되었고, Control plane 운영이 완전 자동화 된다는것

하나만으로도 전환할 가치가 충분했습니다. 

클러스터 버전을 업그레이드 할 떄도 콘솔에서 클릭만하면 컨트롤 플레인의 버전이 자동으로 올라가며, 최근에

쿠버네티스 1.18 버전 이후에는 server-side apply 기능이 추가되면서 kube-proxy, VPC CNI, CoreDNS 등이 에드온으로 관리되어

특별한 주의 귀울이지 않아도 손쉽게 업그레이드 가능해졌습니다.

팀이 빠르게 움직이기 위해서는 이런 관리형 서비스를 도입해 운영코스트를 줄이는것이 중요.

그 외에도 쿠버네티스 클러스터 운영에 필요한 몇몇 중요 포인트 들이 있는데,  팀의 니즈에 따라 적합한 솔루션을 잘 검토하고 도입하기를 추천합니다.

버즈빌에서는 EKS 클러스터를 생성하고 고나리하는 설정은 테라폼을 하고있습니다.

앞에 설명한것처럼 모든 인프라 설정을 테라폼으로 관리하기 떄문입니다.

CLI도구인 eksctl을 활용해도 매우 편리하게 클러스터 관리 가능합니다.

워크로드 요구사항에 따라 클러스터를 확장하고 축소하기 위해 클러스터 오토스케일러를 활용합니다.

비용 절감을 위해 스팟 인스턴스를 적극 활용중입니다. 수명주기가 매우 짧은 컨테이너와 속성과 컨테이너 스케쥴링에 

최적환된 쿠버네티스 궁합을 잘 레버리지 하면 5~70퍼 센트까지 비용절감을 가져갈수 있으니 고려해 보는것도 좋습니다.

애플리케이션 로그는 각 노드에 Daemonset으로 배포된 Fluent-bit이 fluentd로 로그를 포워딩하며, 로그의 성격에 따라

firehose나 kinesis stream으로 유입하여  Redshift 등으로 보내고 있으며  원본 로그를 S3에 직접 적재하기도 합니다.

빠르게 로그를 조회하기 위해 Loki stack 활용하고 있습니다.

로키는 모니터링 도구로 활용중인 GraFana와 Prometheus와 궁합이 잘맞습니다.

클라우드 네이티브 환경에서는 다양한 계층의 모니터링이 필수인데 특히나 요청이 여러 마이크로서비스에 거쳐 발생하기 떄문에 

분산 트레이싱에 대한 고려가 필수적입니다.

Zipkin, Jagger등 여러 도구들이 있으며, 버즈빌에서는 관리 코스트 줄이기 위해 Datadog같은 사용 APM을 적극 도입하여 사용중입니다.

긜고 secret을 서비스 코드베이스에서 분리하고 CI/CD 파이프라인에 노출하지 않기 위해 암호화된형태로 보관하기 위해

sealed-secrets와 AWS secrests manager를 활용하여 배포된 파드에서만 최종적인 secret이 주입될 수 있도록 하였습니다.

각 서비스에서 DynamoDb, S3 등 여러 AWS 서비스에 접근해야 하는경우가 많기 떄문에 파드에 AWS 접근권한ㅇ을 부여하는 방법을 많이 고민하게 되는데

워커노드에 EC2 인스턴스 프로필 이나 AWS Access Key 방식으로 권한을  부여하는것은 추천X

EC2 인스턴스 프로필의 경우 단일 서비스를 배포하느 경우에는 해당 서비스가 필요한 권한만 부여하면 되지만

쿠버네티스의 경우 클러스터에 배포된 서비스 들이 필요로하는 권한을 집합적으로 요청하기 떄문에 최소 권한 모드로 운영하기 어렵다.

엑세스키의 경우 키가 유출되면 외부환경에서도 동일한 권한을 획득할수 있게 되고, 주기적으로 롤링하기도 어렵다.

권장하는 방식은 파드수준에서 IAM Role 권한을 부여하는것입니다.

기존에는 kiam 이나 kube2iam 같은 써드파티 도구로 구성해야 했지만 IAM rols for service account 라고해서

쿠버네티스에서 파드를 실행하는 서비스어카운트에 IAM Role 부여하는 방식을 공식적으로 지원하고 있습니다.

![image](https://user-images.githubusercontent.com/62640332/155570734-107e2c10-f180-4b5c-b3e8-66c01f6f7ae7.png)

서비스 메쉬를 위한 Istio 사용입니다.

쿠버네티스는 MSA 구현하는데 훌륭한 서비스를 제공하지만 서비스 메쉬를 활용하면 트래픽 제어나 보안, 모니터링

측면에서 유리한 부분이 많습니다.

Istio는 모든 파드에 envoy라는 사이드카 프록시를 함께 배치하여  서비스간의 트래픽을 가초애서 다양한 트래픽 관리 기법을 적용할 수 있습니다.

서비스간 호출의 circuit breaker, timeout, retry 등을 service mesh에서 설정하여 코드레벨에서

복잡한 클라이언트 설정을 하지 않아도 어느정도 대응 됩니다.

그리고 요청의 일부분만 다른 목적지로 보내거나, 트래픽을 복제하난 등의 설정을 지원하기 떄문에

카나리 배포,  트래픽 미러링을 통한 요청 검증, A/B 테스트 등을 할수 있습니다.

또한 쿠버네티스에서도 어느정도의 네트워크 Ploicy을 지정할 수 있지만 mTLS나 Authentication, Authoriztion 등의 

고도화된 보안 정책을 정의할 수 있고, 프록시나 서비스 수준에서의 지표를 알아서 수집하고, tracing 헤더를 잘 전달하면

분산트레이싱까지 지원하기 때문에 서비스 가시성을 손쉽게 확보할 수 있습니다. 

다만 Istio는 버전이 빠르게 올라가고 지원기간도 짧은데다  안정적인 운영을 위한 지식 많이 요구되기 떄문에

이러한 세세한 트래픽 ㅗ간리 기능이 필요한지 검토하고, 도입 결정하는것이 좋습니다.

---
ㅁ 레거시 애플리케이션 현대화

![image](https://user-images.githubusercontent.com/62640332/155571642-e78a84c7-4354-4c76-8a83-4420023ba71e.png)

레거시 모노리스 애플리케이션을 일정한 시간을 들여서  한번에 MSA 전환하는것은 많은 비용과 시간이 듭니다.

마이그레이션 하는 과정에서도 새로운 요구사항이 생겨나고, 검증해야하는 포인트도 늘어나기 때문입니다.

따라서 점근적인 접근이 중요합니다. 버즈빌에서는 기존 레거시 애플리케이션을 현재 정의되어있는 형태에서

거의 손대지 않고 컨테이너 환경으로 옮기는 작업을 하였고, 쿠버네티스 운영에 익숙해짐과 동시에 클라우드 네이티브 환경의 다양한

이점을 먼저 경험했습니다. 그리고 난뒤 코드베이스와 배포 파이프라인 설정을 점점 더 클라우드 네이티브 환경에 최적화된 형태로 고쳤고,

그이후에는 활발히 기능이 변경되는 서비스를 파악해서 하나씩 MSA로 분리해나가는 과정을 거쳤습니다.

![image](https://user-images.githubusercontent.com/62640332/155572044-aad4e55c-878b-484a-82af-865b47c94f20.png)

기존에 온프로미스나 EC2 환경에서 운영되는 레거시 워크로드가 있다면 우선은 최소한의 변경만하여 컨테이너기반의 운영환경으로

옮기는 작업을 했습니다. 흔히 그대로 옮긴다하여 Lift and Shift라 불리는 방식입니다.

기존 환경에서는 클라우드 네이티브 환경에서처럼 쉽게 환경을 변경하거나 테스트하기 어렵기 때문에

개선을 하기 위한 비용이 큽니다. 그래서 우선 온프램이나 VM에서 운영하는 환경을 그대로 컨테이너화 해서

쿠버네티스운영 환경에 배포하는것을 목표로 하였습니다.

Dockerfile을 작성해서 puppet 이나 ansible 같은 인스턴스 프로비저닝스크립트에 정의된 시스템 설정값이나 의존성 설치들을 그대로 옮기고 

애플리케이션을 빌드하고 구동하는 커맨드 등도 확인해서 컨테이넘나 실행하면 기존 운영환경 워크로드 실행 될수 있또록 합니다.

워크로드를 쿠버네티스에 배포하기 위해서는  Deployment, Service 등의 manifest를 yaml 이나 json등으로 작성해야 합니다.

여러 환경에 대해 반복적으로 정의하게 되므로 helm이나 kustomize 같은 템플릿 엔진을 많이 활용합니다.

버즈빌에서는 서비스별로 helm chart를 작성해서 배포하고 있습니다.

이 떄 기존 운영환경의 방화벽 설정, 보안그룹 등을 확인해서 네트워크 인터페이스를 구버네티스 Deployment의 포트나 서비스 정의에 반영합니다.

만흥ㄴ 경우 nginx 나 haproxy 같은 reverse proxy나 로그 포워딩 에이전트, 모니터링 에이전트 등이 같은 서버에서 실행되고 있을것입니다.

최초 마이그레이션 떄는 영향도 최소화 하기 위해서 이러한 보조 서비스에대한 설정을 그대로 컨테이너화해서 함께 배포하는것 권장

![image](https://user-images.githubusercontent.com/62640332/155573009-db6c4b2c-69be-46ea-b8fd-45fcef4d9bc8.png)

EC2나 온프램 환경에서 배포된 서비스와 런타임의 설정을 최대한 식별한후 그대로 컨테이너화 해서 쿠버네티스에 배포하는것

reverse proxy나 fluent 같은 로깅 agent 등은 메인 서비스 컨테이너와 동일한 파드 내부에 별도 컨테이너로 저으이하면 됩니다.

이러한 형태를 "사이드카 패턴" 이라고 합니다.

독립적인 컨테이너 이지만, 볼륨이나 네트워크 인터페이스를 같은 파드 내부에서는 공유하기 떄문에 파드 하나가 기존의 VM 이나 물리서버처럼

하나의 호스트에 배포된것 처럼 됩니다. 이렇게 쿠버네티스에 레거시 서비스가 배포되고나면 점진적으로 트래픽을 쿠버네티스로

유입시키면서 동작을 검증하면 됩니다. 버즈빌의 경우 router53의 Weighted routing을 활용하여 기존 EC2 타겟그룹에 연결돼 있던 ELB에서

쿠버네틱스의 Ingress Controller와 연결된 ELB로 트래픽을 점진적을 옮겼습니다.

이렇게 옮기고 나면, 배포는 쿠버네틱스의 deployment의 컨테이너 이미지만 신규 이미지 태그로 지정해주면 알아서 롤링이 되고, 

파드를 cpu등의 리소스에 따라 오토스케일링해주는 HPA를 활용할 수 있는 등 컨테이너 오케스트레이션 장점을 즉각적으로 누릴 수 있게 됩니다.

![image](https://user-images.githubusercontent.com/62640332/155574004-5cf93511-ba94-412e-9f9f-cf0bd0cb402e.png)

그리고 클라우드 네이티브 환경에서 운영과 모니터링을 하면서 점진적인 개선을 하며, 이때 PaaS 업체로 유명한 Heroku에서 제안한

Twelve-Factor App 접근방식을 고민해보면 좋습니다. 손쉽게 이식 가능하고 클라우드에 최적화된 애플리케이션을

개발할떄 고민하면 좋은 12가지 원칙을 제안하고 있기 떄문에, 해당 원칙을 지키기 위해서 노력하면 자연스럽게 클라우드 네이티브 환경에 

최적화된 애플리케이션을 구성할수 있습니다.

몇가지 원칙을 살펴보면

1. 단일 코드베이스를 컨테이너화 하여 여러 환경에 배포하고 개발이나 프로덕션 등 환경별로 달라지는 설정은 코드에 넣지 않고

환경마다 환경변수나 설정파일 등으로 주입하는 것을 추천합니다.

2. 프로세스는 상태 관리를 최소화해서  가능하면 Persistnet Volume 등을 활용하지 않고, 만약 상태 피룡시 별도 DB를 활용합니다.

\# 쿠버네틱스에서는 단일 컨테이너는 가능한 단일 프로세스를 실행하도록 해서 하나의 프로세스가 다양한 프로세스를 관리하는 방식은 지양하는것이 좋습니다.

3. 다른 서비스가 접근시 미리 정의된 포트로 접근하는것이 좋음

4.  컨테이너 생성/삭제가 매우 빈번하게 일어나기 떄문에 애플리케이션이 빠르게 구동되고 종료되도록 개선을하며, 종료시에는 os 
   
시그널처리하여 실행중이던 작업이 안전하게 마무리 되는 것을 보장합니다.

\# 이떄 DB를 포함한 각종 클라이언트 커낵션리소스를 잘 해재하도록 코드를 개선합니다.

5. 레거시 시스템은 로그를 파일로 남기는 경우가 많은데, 쿠버네틱스에서는 로그를 stdout이나 stderr 스트림에 이벤트 형태로 남겨서

중앙 집중화된 로그 관리 방식을 가져가는 것을 선호합니다.

![image](https://user-images.githubusercontent.com/62640332/155575219-9040aaf7-7d4c-40bc-bd79-17d30600cb04.png)

기존에 sidecar 컨테이너로 배포되었던 nginx 설정은 쿠버네티스 Ingress 리소스나 Istio의 Virtualservice 등으로 옮겨왔습니다.

서비스가 파일시스템에 생성한 로그를 포워딩하기 위해서 fluentd같은 로그 에이전트를 사이드카로 배포 했는데요,

이제 로그는 파이릿스템이 아닌 stout에 출력하게 되고 노드나 클러스터수준의 로그 관리 시스템이 로그를 취합하여

별도 저장소로 포워딩하는 형태로 관리르 하기 시작합니다. 각 환경별로 달라질 수 있는 환경변수나 인증서 파일등은

ConfigMap이나 Secret으로 정의한 뒤 파드에 환경변수나 볼륨에 마운트하고 있습니다.

이렇게 시간이 지남에따라 점차 기존 모노리스는  클라우드 네이티브 환경에 최적화된 애플리케이션으로 변하게 됩니다.

![image](https://user-images.githubusercontent.com/62640332/155575787-5d979094-8664-42c7-aaf9-d882035b2314.png)

ㅁ 모니리스 서비스에서 bounded context를 파악하고 도메인을 분리해 나가는것입니다.

먼저 모듈이나 네임스페이스 수준에서 리팩토링하여 각 도메인이 충분히 디커플링 되도록 합니다.

특히 RDBMS를 활용해 개발된 경우 모듈간 테이블 join등을 많이 쓸 수 있는데 이런 부분 제거하고, 테이블간 의존성 분리해 나갑니다.

디커블링 어느정도 된 모듈들 중 중요도가 높거나 활발히 개발되는 모델이 속한 도메인을 우선 MSA 작성합니다.

이떄 로직을 그대로 포딩한다기 보다는 해당 모듈이 호출되는 인터페이스는 유지하면서 내부 구조를 새로운 코드로 작성 권장합니다.

버즈빌에서는 새로 분리된 MSA에는 클린 아키텍처를 적용하는 것을 권장하고 있습니다.

MSA개발이 완료되었다면 트래픽 미러링이나 카나리 방식을 통해 트래픽을 점진적으로 신규 MSA로 유입하게 됩니다.


![image](https://user-images.githubusercontent.com/62640332/155576384-9336832e-6717-4160-8a7e-918f93e4550d.png)

쿠버네티스에 이식해서 잘 운영중인 모놀리스 서비스X 가 있습니다.

최근에 모듈C가 활발히 개발되다보니 레거시 구조를 가져가기 보다는 신규MSA 분리해서 운영하기로 결정

![image](https://user-images.githubusercontent.com/62640332/155576548-ccf2beb9-26f7-48d0-a136-1de96ffa52f9.png)

그렇게해서 기존의 모듈 C를 잘 재작성하여 서비스 Y를 만들어 배포하는것 

서비스 Y는 기존서비스 X의 모듈 C에 대해 수행하던 대부분의 유닛테스트와 통합테스트를 옮겨왔고, 테스트는 성공적으로 통과

그러나  서비스Y로 바로 프로덕션 수준의 트래픽을 옮기기엔 불안합니다.

실제 트래픽에는 기존 테스트 케이스가 커버하지 못하는 엣지 케이스가 분명 있을수 있기 떄문입니다.

그리고 갑자기 부하가 많이 발생시 충분히 scaling 되는지 익셉션 이나 에러율에 대한 모니터링이 잘되는지 등등은

잘 정의된 몇몇 테스트 케이스나 요청으로 검증하기 어렵습니다.

![image](https://user-images.githubusercontent.com/62640332/155576990-e69b824f-e799-449a-b99b-341d78465bc3.png)

버즈빌에서는 Istio의 Viretualservice를 활용해 먼저 일부 트래픽을 샘플링해서 Y로 미러링하는 방법을 이용했습니다.

트래픽을 미러링하면 사용자느 기존 서비스 X의 응답을 받지만 실제 요청과 동일한 shadow 트래픽이 서비스 Y로 유입됩니다.

미러링된 요청에 대한 서비스 Y의 응답은 버려지지만, 올바른 결과를 내는지 로깅을 하거나  side effect가 있는 요청이라면

shadow DB등에 데이터를 쓴 다음 기존 서비스X DB와 비교를 하게 됩니다.

이런식으로 미러링을 통해 충분히 검증이 끝나면 다음은 카나리 방식으로 실제 트래픽의 일부를 서비스 Y에 흘려보냅니다.

![image](https://user-images.githubusercontent.com/62640332/155577267-7856339a-fd64-4bc4-bde7-088a231bd335.png)

예를들어 virtualservice에서 /bar라는 경로로 보내는 트래픽에 대해 20% 가중치는 서비스 Y로 나머지는 서비스 X로 보내개 합니다.

![image](https://user-images.githubusercontent.com/62640332/155577545-9a55f8bd-ade3-4e99-8431-39470ae824e3.png)

일부 트래픽만 신규 서비스 처리했을때 문제가 없다면 최종적으로 /bar로 유입되는 모든 트래픽을 서비스 Y로 전송하면  MSA 분리 완료

